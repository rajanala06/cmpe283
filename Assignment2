diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c
index fd3951638ae4..7d88c693ff43 100644
--- a/arch/x86/kvm/cpuid.c
+++ b/arch/x86/kvm/cpuid.c
@@ -12,6 +12,9 @@
  *
  */
 
+//#include <asm-generic/atomic.h>
+//#include <asm-generic/atomic-long.h>
+#include <linux/atomic.h>
 #include <linux/kvm_host.h>
 #include <linux/export.h>
 #include <linux/vmalloc.h>
@@ -924,7 +927,14 @@ static struct kvm_cpuid_entry2* check_cpuid_limit(struct kvm_vcpu *vcpu,
 	}
 	return kvm_find_cpuid_entry(vcpu, maxlevel->eax, index);
 }
-
+/*
+ * this source code goes to vmx.ko, make it into kernel
+ */
+atomic_t num_exits = ATOMIC_INIT(0);
+atomic_long_t num_cycles = ATOMIC_INIT(0);
+EXPORT_SYMBOL(num_exits);
+EXPORT_SYMBOL(num_cycles);
+	
 bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 	       u32 *ecx, u32 *edx, bool check_limit)
 {
@@ -932,6 +942,20 @@ bool kvm_cpuid(struct kvm_vcpu *vcpu, u32 *eax, u32 *ebx,
 	struct kvm_cpuid_entry2 *best;
 	bool entry_found = true;
 
+	/*only for CMPE283 assignment 02:
+	 * as port for reporting num_exit and num_cycles in exit.
+	 * using extern variable, to be improved.
+	 * */
+	if (function == 0x4FFFFFFF) {
+		*eax = (u32) atomic_read(&num_exits);
+		*edx = 0;
+		unsigned long tmp = (u64) atomic_long_read(&num_cycles);
+		*ecx = (u32) (0xFFFFFFFFLL & tmp);//low 32 bit
+		*ebx = (u32) ((0xFFFFFFFF00000000LL & tmp) >> 32); // high 32 bit
+
+		return entry_found;
+	}
+
 	best = kvm_find_cpuid_entry(vcpu, function, index);
 
 	if (!best) {
diff --git a/arch/x86/kvm/vmx/vmx.c b/arch/x86/kvm/vmx/vmx.c
index ab432a930ae8..e3ba8d7c2091 100644
--- a/arch/x86/kvm/vmx/vmx.c
+++ b/arch/x86/kvm/vmx/vmx.c
@@ -16,6 +16,10 @@
  *
  */
 
+//#include <asm-generic/atomic.h>
+//#include <asm-generic/atomic-long.h>
+//#include <asm-generic/atomic-instrumented.h>
+#include <linux/atomic.h>
 #include <linux/frame.h>
 #include <linux/highmem.h>
 #include <linux/hrtimer.h>
@@ -5749,8 +5753,27 @@ static void dump_vmcs(void)
  * The guest has exited.  See if we can fix it or if we need userspace
  * assistance.
  */
+/*Mars local helper*/
+static uint64_t __rdtsc(void){
+    unsigned int lo,hi;
+    __asm__ __volatile__ ("rdtsc" : "=a" (lo), "=d" (hi));
+    return ((uint64_t)hi << 32) | lo;
+}
+
+/*
+ * global variable by Mars, this source code goes to vmx_intel.ko
+ * , will be built as a module
+ * */
+extern atomic_t num_exits;
+extern atomic_long_t num_cycles;
+
 static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 {
+	unsigned long long start_cycle, stop_cycle;
+        start_cycle = __rdtsc(); 
+
+	atomic_fetch_add(1, &num_exits);
+
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	u32 exit_reason = vmx->exit_reason;
 	u32 vectoring_info = vmx->idt_vectoring_info;
@@ -5768,17 +5791,36 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 		vmx_flush_pml_buffer(vcpu);
 
 	/* If guest state is invalid, start emulating */
-	if (vmx->emulation_required)
-		return handle_invalid_guest_state(vcpu);
+	if (vmx->emulation_required){
+
+//		return handle_invalid_guest_state(vcpu);
+//		/*
+		int tmp = handle_invalid_guest_state(vcpu);
+		stop_cycle = __rdtsc();
+		atomic64_fetch_add((stop_cycle - start_cycle), &num_cycles);
+		return tmp;
+//		*/
+	}
+
+	if (is_guest_mode(vcpu) && nested_vmx_exit_reflected(vcpu, exit_reason)){
 
-	if (is_guest_mode(vcpu) && nested_vmx_exit_reflected(vcpu, exit_reason))
-		return nested_vmx_reflect_vmexit(vcpu, exit_reason);
+//		return nested_vmx_reflect_vmexit(vcpu, exit_reason);
+//		/*
+		int tmp = nested_vmx_reflect_vmexit(vcpu, exit_reason);
+		stop_cycle = __rdtsc();
+		atomic64_fetch_add((stop_cycle - start_cycle), &num_cycles);
+		return tmp;
+//		*/
+	}
 
 	if (exit_reason & VMX_EXIT_REASONS_FAILED_VMENTRY) {
 		dump_vmcs();
 		vcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 		vcpu->run->fail_entry.hardware_entry_failure_reason
 			= exit_reason;
+
+		stop_cycle = __rdtsc();
+		atomic64_fetch_add((stop_cycle - start_cycle), &num_cycles);
 		return 0;
 	}
 
@@ -5786,6 +5828,9 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 		vcpu->run->exit_reason = KVM_EXIT_FAIL_ENTRY;
 		vcpu->run->fail_entry.hardware_entry_failure_reason
 			= vmcs_read32(VM_INSTRUCTION_ERROR);
+
+		stop_cycle = __rdtsc();
+		atomic64_fetch_add((stop_cycle - start_cycle), &num_cycles);
 		return 0;
 	}
 
@@ -5812,6 +5857,9 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 			vcpu->run->internal.data[3] =
 				vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 		}
+
+		stop_cycle = __rdtsc();
+		atomic64_fetch_add((stop_cycle - start_cycle), &num_cycles);
 		return 0;
 	}
 
@@ -5835,12 +5883,22 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 	}
 
 	if (exit_reason < kvm_vmx_max_exit_handlers
-	    && kvm_vmx_exit_handlers[exit_reason])
-		return kvm_vmx_exit_handlers[exit_reason](vcpu);
+	    && kvm_vmx_exit_handlers[exit_reason]) {
+		int tmp = kvm_vmx_exit_handlers[exit_reason](vcpu);
+		
+		stop_cycle = __rdtsc();
+		atomic64_fetch_add((stop_cycle - start_cycle), &num_cycles);
+		
+		return tmp;
+	}
 	else {
 		vcpu_unimpl(vcpu, "vmx: unexpected exit reason 0x%x\n",
 				exit_reason);
 		kvm_queue_exception(vcpu, UD_VECTOR);
+
+		stop_cycle = __rdtsc();
+		atomic64_fetch_add((stop_cycle - start_cycle), &num_cycles);
+		
 		return 1;
 	}
 }
